{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "id": "7u5uDAEg51YR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bike_dataset = pd.read_csv('/content/drive/MyDrive/hour.csv') # Load the dataset\n",
        "\n",
        "X = bike_dataset.drop(columns=['dteday', 'cnt'])\n",
        "Y = bike_dataset['cnt'] # The target variable\n",
        "print(X.head())\n",
        "print(Y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztyiKbyU6K9D",
        "outputId": "eb82b15d-7367-4af9-809f-fff8a2f745d1"
      },
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   season  yr  mnth  hr  holiday  weekday  workingday  weathersit  temp  \\\n",
            "0       1   0     1   0        0        6           0           1  0.24   \n",
            "1       1   0     1   1        0        6           0           1  0.22   \n",
            "2       1   0     1   2        0        6           0           1  0.22   \n",
            "3       1   0     1   3        0        6           0           1  0.24   \n",
            "4       1   0     1   4        0        6           0           1  0.24   \n",
            "\n",
            "    atemp   hum  windspeed  \n",
            "0  0.2879  0.81        0.0  \n",
            "1  0.2727  0.80        0.0  \n",
            "2  0.2727  0.80        0.0  \n",
            "3  0.2879  0.75        0.0  \n",
            "4  0.2879  0.75        0.0  \n",
            "0    16\n",
            "1    40\n",
            "2    32\n",
            "3    13\n",
            "4     1\n",
            "Name: cnt, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, X_temp, Y_test, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42) # Splitting the dataset\n",
        "X_val, X_train, Y_val, Y_train = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "6z5u3S667UAU"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2, min_samples_leaf=1):\n",
        "      # Initialize hyperparameters for the tree\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.tree = None\n",
        "\n",
        "    class Node:\n",
        "        __slots__ = ('val', 'feat', 'thresh', 'left', 'right')\n",
        "        def __init__(self, val=None, feat=None, thresh=None, left=None, right=None):\n",
        "          # Node stores either a value (leaf) or a splitting feature and threshold\n",
        "            self.val = val\n",
        "            self.feat = feat      # Feature index for splitting\n",
        "            self.thresh = thresh  # Threshold value for splitting\n",
        "            self.left = left\n",
        "            self.right = right\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        X, Y = np.array(X), np.array(Y)\n",
        "        self.tree = self.build_tree(X, Y, 0) # start building from root at depth 0\n",
        "\n",
        "    def build_tree(self, X, Y, depth):\n",
        "        n_samples, n_features = X.shape\n",
        "        if (depth >= self.max_depth or n_samples < self.min_samples_split or len(np.unique(Y)) == 1):\n",
        "            return self.Node(val=np.mean(Y))  # return a leaf node with mean value\n",
        "\n",
        "        # Vectorized best split\n",
        "        best_feat, best_thresh, best_mse = None, None, np.inf\n",
        "        for f in range(n_features):\n",
        "            sort_idx = np.argsort(X[:, f])  # Sort feature values\n",
        "            Xf, Yf = X[sort_idx, f], Y[sort_idx]\n",
        "            cumsum_y = np.cumsum(Yf)\n",
        "            cumsum_y2 = np.cumsum(Yf**2)\n",
        "            n = len(Yf)\n",
        "            split_points = np.where(Xf[1:] != Xf[:-1])[0] + 1 # Candidate split points are where feature values change\n",
        "            left_count = split_points\n",
        "            right_count = n - left_count\n",
        "            left_mask = (left_count >= self.min_samples_leaf) & (right_count >= self.min_samples_leaf)\n",
        "            split_points = split_points[left_mask]\n",
        "\n",
        "            if len(split_points) == 0: # No valid splits for this feature\n",
        "                continue\n",
        "\n",
        "            for sp in split_points:\n",
        "                left_csum, left_csum2 = cumsum_y[sp-1], cumsum_y2[sp-1]\n",
        "                left_n = sp\n",
        "                right_csum, right_csum2 = cumsum_y[-1] - left_csum, cumsum_y2[-1] - left_csum2\n",
        "                right_n = n - sp\n",
        "                left_mse = (left_csum2 - (left_csum**2)/left_n)/left_n\n",
        "                right_mse = (right_csum2 - (right_csum**2)/right_n)/right_n\n",
        "                weighted_mse = (left_mse*left_n + right_mse*right_n)/n\n",
        "\n",
        "                if weighted_mse < best_mse:\n",
        "                    best_mse = weighted_mse\n",
        "                    best_feat = f\n",
        "                    best_thresh = (Xf[sp-1] + Xf[sp])/2 # Split threshold is midpoint\n",
        "\n",
        "        # if no valid split found\n",
        "        if best_feat is None:\n",
        "            return self.Node(val=np.mean(Y))\n",
        "\n",
        "        # partition data according to the best split\n",
        "        left_mask = X[:, best_feat] <= best_thresh\n",
        "        right_mask = ~left_mask\n",
        "        left = self.build_tree(X[left_mask], y[left_mask], depth+1)\n",
        "        right = self.build_tree(X[right_mask], y[right_mask], depth+1)\n",
        "        return self.Node(feat=best_feat, thresh=best_thresh, left=left, right=right)\n",
        "\n",
        "    def predict(self, X): # prediction using a stack to avoid recursion\n",
        "        X = np.array(X)\n",
        "        preds = np.empty(X.shape[0], dtype=float)\n",
        "        stack = [(np.arange(X.shape[0]), self.tree)]\n",
        "\n",
        "        while stack:\n",
        "            idxs, node = stack.pop()\n",
        "            if node.val is not None:\n",
        "                preds[idxs] = node.val\n",
        "                continue\n",
        "            left_idx = idxs[X[idxs, node.feat] <= node.thresh]\n",
        "            right_idx = idxs[X[idxs, node.feat] > node.thresh]\n",
        "            if len(left_idx) > 0:\n",
        "                stack.append((left_idx, node.left))\n",
        "            if len(right_idx) > 0:\n",
        "                stack.append((right_idx, node.right))\n",
        "        return preds"
      ],
      "metadata": {
        "id": "VSaGffQg8_Sz"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search(X_train, Y_train, X_val, Y_val, max_depth_range, min_samples_split_range, min_samples_leaf_range, model_class):\n",
        "  best_parameters = None # track best hyperparameters\n",
        "  best_score = float(\"inf\")\n",
        "\n",
        "  param_grid = itertools.product(max_depth_range, min_samples_split_range, min_samples_leaf_range)\n",
        "\n",
        "  for depth, split, leaf in param_grid:\n",
        "      model = model_class(max_depth=depth, min_samples_split=split, min_samples_leaf=leaf)\n",
        "      model.fit(X_train, Y_train) # Fit the model on training data\n",
        "      prediction = model.predict(X_val)\n",
        "      mse = mean_squared_error(Y_val, prediction) # compute mean squared error\n",
        "\n",
        "      if mse < best_score :\n",
        "        best_score = mse\n",
        "        best_parameters = (depth, split, leaf)\n",
        "\n",
        "  return best_parameters, best_score"
      ],
      "metadata": {
        "id": "o38AHzEY103D"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters, best_score = grid_search(X_train, Y_train, X_val, Y_val, range(10, 19), range(7, 13), range(2, 8), DecisionTree)\n",
        "\n",
        "print(f\"Best Parameters : Max Depth={best_parameters[0]}, Min Samples Split={best_parameters[1]}, Min Samples Leaf={best_parameters[2]}\") #print the best hyperparameters found\n",
        "print(f\"Best MSE : {best_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "rp0jlSLQ2WNM",
        "outputId": "0cb1e177-2525-4d8f-a758-ece34b91d984"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1813029622.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best Parameters : Max Depth={best_parameters[0]}, Min Samples Split={best_parameters[1]}, Min Samples Leaf={best_parameters[2]}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#print the best hyperparameters found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best MSE : {best_score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-528777357.py\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(X_train, Y_train, X_val, Y_val, max_depth_range, min_samples_split_range, min_samples_leaf_range, model_class)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Fit the model on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compute mean squared error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1875737765.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# start building from root at depth 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1875737765.py\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(self, X, Y, depth)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mleft_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mbest_thresh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mright_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mleft_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_thresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scratch_model = DecisionTree(\n",
        "    max_depth=best_parameters[0],\n",
        "    min_samples_split=best_parameters[1],\n",
        "    min_samples_leaf=best_parameters[2]\n",
        ")\n",
        "scratch_model.fit(X_train, Y_train) #fit the model on the training data\n",
        "scratch_pred = scratch_model.predict(X_test)\n",
        "scratch_mse = mean_squared_error(Y_test, scratch_pred) # Evaluate the model's performance\n",
        "scratch_r2 = r2_score(Y_test, scratch_pred)\n",
        "\n",
        "print(\"Test MSE: \", scratch_mse)\n",
        "print(\"Test R2: \", scratch_r2)"
      ],
      "metadata": {
        "id": "5cdZrzKR5TFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(Y_test, scratch_pred)\n",
        "plt.plot([Y_test.min(), Y_test.max()],\n",
        "         [Y_test.min(), Y_test.max()],\n",
        "         \"r--\", lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted Values\")\n",
        "plt.show() # Plot the graph"
      ],
      "metadata": {
        "id": "i1RnHKRZ66jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters_scikit, best_score_scikit = grid_search(\n",
        "    X_train, Y_train, X_val, Y_val,\n",
        "    range(10, 19), range(7, 13), range(2, 8),\n",
        "    DecisionTreeRegressor\n",
        ")\n",
        "\n",
        "print(\"Best Hyperparameters (Scikit): \", best_parameters_scikit)\n",
        "print(\"Best Validation MSE (Scikit): \", best_score_scikit)\n",
        "\n",
        "scikit_model = DecisionTreeRegressor(\n",
        "    max_depth=best_parameters_scikit[0],\n",
        "    min_samples_split=best_parameters_scikit[1],\n",
        "    min_samples_leaf=best_parameters_scikit[2],\n",
        "    random_state=42\n",
        ")\n",
        "scikit_model.fit(X_train, Y_train) # create a scikit-learn DecisionTreeRegressor\n",
        "scikit_pred = scikit_model.predict(X_test)\n",
        "scikit_mse = mean_squared_error(Y_test, scikit_pred)\n",
        "\n",
        "print(\"Scratch Tree Test MSE:\", scratch_mse) # print and compare test MSEs of scratch and scikit-learn\n",
        "print(\"Scikit-learn Tree Test MSE:\", scikit_mse)"
      ],
      "metadata": {
        "id": "F7CW9Mr56PZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(Y_test, scikit_pred)\n",
        "plt.plot([Y_test.min(), Y_test.max()],\n",
        "         [Y_test.min(), Y_test.max()],\n",
        "         \"r--\", lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted Values (Decision Tree)\")\n",
        "plt.show() # Plot the graph"
      ],
      "metadata": {
        "id": "LtSzp_jK7YrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "plot_tree(\n",
        "    scikit_model,\n",
        "    filled=True,\n",
        "    feature_names=X_train_df.columns,\n",
        "    rounded=True,\n",
        "    fontsize=10\n",
        ")\n",
        "\n",
        "plt.show() # Printing the tree\n"
      ],
      "metadata": {
        "id": "4B-eVOZHZ7VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We observe from the MSE scores :\n",
        "\n",
        "\n",
        "*   Scratch Tree Test MSE: 5754.788112277918\n",
        "*   Scikit-learn Tree Test MSE: 5727.479907594079\n",
        "\n",
        "The custom Decision Tree works correctly and produces results comparable to scikit-learnâ€™s professional implementation. For real-world tasks, scikit-learn is preferred due to speed and optimizations.\n",
        "The difference is expected because scikit-learn is highly optimized in C and uses better numerical stability tricks.\n"
      ],
      "metadata": {
        "id": "LFwe2-LUt2N7"
      }
    }
  ]
}